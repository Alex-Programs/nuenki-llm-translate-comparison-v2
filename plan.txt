- Better execution format. We use an inheritance based system to define how we translate things in a structured, ordered way.
- Better distinction of service providers.
- More robust latency handling by running it on a server with a dedicated line.
- Caching, obviously. Heaps of it.
- Put everything in one program this time, rather than chaining.
- Bradley-Terry
- Seperate text datasets as well as separate languages
- Also store data separately for different evaluators
- Separate, controlled tests for:
Temperature
Language

Actually. Hmm. Can I combine them?

Option A:
- Iterate over languages
- Iterate over model pairs
- Iterate over temperatures
- Translate with each sentence

That gives me:
- What is best, by language
- What is best, in general (just expand the frame)
- Best temperatures (just change what we think about)



Option B:
- Define an MMR for temp and an MMR for
... yeah this won't be statistically independent





Thinking about the algo:
- each unit of inter-language comparison between models must occur at all 3 temps and all sentences
- this means you can fairly look at per-sentence-category difficulties, per temps, etc
- actual comparisons should be on an interleaved system, where we carefully choose the order for maximum delta
- Also do anchor ones - highest to lowest, quartile to quartile, median-1 to highest+1, median+1 to lowest-1

questions to ask:
- are LLMs biast towards themselves? (probably best to just make "leaderboard if only x participated" with deltas)
- which temps work best? 
- best LLMs (take the mean of per-language, and have a disclaimer)
- best LLMs at each language